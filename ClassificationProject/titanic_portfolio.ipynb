{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project utilizes the Titanic Dataset from the Kaggle Titanic competition. The goal is to explore the data and create a model to predict the survival of an individual based on the given features or those that are engineered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1. Import Libraries and Acquire Data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-05-29T04:39:15.230640Z",
     "iopub.status.busy": "2022-05-29T04:39:15.229858Z",
     "iopub.status.idle": "2022-05-29T04:39:28.709798Z",
     "shell.execute_reply": "2022-05-29T04:39:28.708877Z",
     "shell.execute_reply.started": "2022-05-29T04:39:15.230534Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data analysis and cleaning\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cycler\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "# !pip install --upgrade ptitprince\n",
    "import ptitprince as pt\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb \n",
    "from sklearn import metrics\n",
    "\n",
    "# Default Styling\n",
    "colors_default = ['#17869E', '#D35151', '#e5ae38', '#6d904f', '#264D58', '#E9DAB4']\n",
    "colors_highlight = ['#51C4D3', '#fc4f30','#ffd966', '#9AE19D', '#008fd5']\n",
    "darks = [\"#1F1F1F\", \"#313131\", '#636363', '#AEAEAE', '#DADADA']\n",
    "colors = cycler('color',colors_default)\n",
    "style=plt.style.use('fivethirtyeight')\n",
    "plt.rc('axes', prop_cycle=colors)\n",
    "plt.rc('lines', linewidth=1)\n",
    "plt.rc('axes', labelsize=14, titlesize=14)\n",
    "plt.rc('font', size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-05-29T04:39:35.698739Z",
     "iopub.status.busy": "2022-05-29T04:39:35.698426Z",
     "iopub.status.idle": "2022-05-29T04:39:35.783627Z",
     "shell.execute_reply": "2022-05-29T04:39:35.782769Z",
     "shell.execute_reply.started": "2022-05-29T04:39:35.698704Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "combined_df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "\n",
    "print('\\nTest Dataframe:')\n",
    "test_df.info()\n",
    "print('='*45)\n",
    "print('\\nTrain Dataframe:')\n",
    "train_df.info()\n",
    "print('\\n')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>2. Data Cleaning</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-29T04:39:36.784196Z",
     "iopub.status.busy": "2022-05-29T04:39:36.783940Z",
     "iopub.status.idle": "2022-05-29T04:39:36.842047Z",
     "shell.execute_reply": "2022-05-29T04:39:36.841175Z",
     "shell.execute_reply.started": "2022-05-29T04:39:36.784168Z"
    }
   },
   "outputs": [],
   "source": [
    "for df in [train_df, test_df]:\n",
    "    # Age null values will be imputed with the mean Age of each Pclass due to their high correlation (shown later)\n",
    "    Age_mean = df.groupby('Pclass')['Age'].mean()\n",
    "    df['Age'] = df.apply(\n",
    "        lambda row: Age_mean[row['Pclass']]\n",
    "        if np.isnan(row['Age'])\n",
    "        else row['Age'], axis=1\n",
    "    )\n",
    "    \n",
    "    # Fare null values will be imputed with the mean Age of each Pclass due to their high correlation (shown later)\n",
    "    Fare_mean = df.groupby('Pclass')['Fare'].mean()\n",
    "    df['Fare'] = df.apply(\n",
    "        lambda row: Fare_mean[row['Pclass']]\n",
    "        if np.isnan(row['Fare'])\n",
    "        else row['Fare'], axis=1\n",
    "    )\n",
    "    \n",
    "    # Embarked null values will be imputed based on the mode\n",
    "    Mode = df['Embarked'].mode()[0]\n",
    "    df['Embarked'].fillna(Mode, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 2.2 Create:</h4> Feature Engineering for train and test/validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-29T04:39:37.592033Z",
     "iopub.status.busy": "2022-05-29T04:39:37.591744Z",
     "iopub.status.idle": "2022-05-29T04:39:37.663808Z",
     "shell.execute_reply": "2022-05-29T04:39:37.662750Z",
     "shell.execute_reply.started": "2022-05-29T04:39:37.592001Z"
    }
   },
   "outputs": [],
   "source": [
    "for df in [train_df, test_df, combined_df]:    \n",
    "    # Column indicating total family size\n",
    "    df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n",
    "    \n",
    "    # Column indicating if passenger has immediate family aboard\n",
    "    df['FamilyAboard'] = 0 # Initialize to No/0\n",
    "    df['FamilyAboard'].loc[df['FamilySize'] > 1] = 1 # Update to Yes/1 if family size is greater than 1\n",
    "    \n",
    "    # Fare Bins/Buckets using qcut or frequency bins\n",
    "    df['FareBin'] = pd.qcut(df['Fare'], 5)\n",
    "    \n",
    "    # Age Bins/Buckets using cut or value bins: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html\n",
    "    df['AgeBin'] = pd.cut(x=df['Age'], bins=[0,18,30,40,50,65,85])\n",
    "   \n",
    "    # Create a column for the passenger's title -- http://www.pythonforbeginners.com/dictionary/python-split\n",
    "    df['Title'] = df['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n",
    "        \n",
    "# Cleanup rare title names\n",
    "stat_min = 10 # While 'rare' is arbitrary, we'll use a common minimum in statistics\n",
    "              # http://nicholasjjackson.com/2012/03/08/sample-size-is-10-a-magic-number/\n",
    "title_names = (combined_df.loc[:,'Title'].value_counts() < stat_min) # Creates true/false series with title as index\n",
    "    \n",
    "for df in [train_df, test_df]:\n",
    "    df['Title'] = df.loc[:,'Title'].apply(lambda x: 'Misc' if title_names.loc[x] == True else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-29T04:39:37.960028Z",
     "iopub.status.busy": "2022-05-29T04:39:37.959740Z",
     "iopub.status.idle": "2022-05-29T04:39:37.969106Z",
     "shell.execute_reply": "2022-05-29T04:39:37.968443Z",
     "shell.execute_reply.started": "2022-05-29T04:39:37.959997Z"
    }
   },
   "outputs": [],
   "source": [
    "# PassengerId, Ticket, Name columns will be dropped because they are arbitrary (title already extracted)\n",
    "# Cabin column will be dropped because it is missing a high portion of data\n",
    "for df in [train_df, test_df]:\n",
    "    df.drop(['Name', 'Ticket', 'Cabin'], axis=1, inplace=True)\n",
    "train_df.drop('PassengerId',axis=1, inplace=True) # Column kept in test_df for submission "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>3. Exploratory Data Analysis</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: There is some redundancy in the EDA charts due to exercising a variety of plotting methods and chart types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-29T04:39:38.858902Z",
     "iopub.status.busy": "2022-05-29T04:39:38.858635Z",
     "iopub.status.idle": "2022-05-29T04:39:38.941197Z",
     "shell.execute_reply": "2022-05-29T04:39:38.940313Z",
     "shell.execute_reply.started": "2022-05-29T04:39:38.858872Z"
    }
   },
   "outputs": [],
   "source": [
    "# Discrete Variable Correlation by Survival using group by aka pivot table:\n",
    "# https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\n",
    "for col in train_df:\n",
    "    if (train_df[col].dtype != 'float64') and (col != 'Survived'):\n",
    "        print('Survival Correlation by:', col)\n",
    "        print(train_df[[col, 'Survived']].groupby(col, as_index=False).mean().sort_values(by='Survived', ascending=False))\n",
    "        print('-'*10, '\\n')      \n",
    "        \n",
    "# using crosstabs: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.crosstab.html\n",
    "print(pd.crosstab(train_df['Title'], train_df['Survived']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-29T04:39:39.141527Z",
     "iopub.status.busy": "2022-05-29T04:39:39.140803Z",
     "iopub.status.idle": "2022-05-29T04:39:40.530499Z",
     "shell.execute_reply": "2022-05-29T04:39:40.529915Z",
     "shell.execute_reply.started": "2022-05-29T04:39:39.141478Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is one method to create subplots, but the prefered method is used in upcoming visualizations\n",
    "plt.figure(figsize=[12,8])\n",
    "\n",
    "plt.subplot(231)\n",
    "sns.boxplot(x=train_df['Survived'], y=train_df['Fare'], showmeans = True, meanline = True, fliersize=4)\n",
    "plt.ylabel('Fare')\n",
    "plt.xlabel('')\n",
    "plt.xticks(ticks=[0,1], labels=['Died','Survived'])\n",
    "\n",
    "plt.subplot(232)\n",
    "sns.boxplot(x=train_df['Survived'], y=train_df['Age'], showmeans = True, meanline = True, fliersize=4)\n",
    "plt.ylabel('Age')\n",
    "plt.xlabel('')\n",
    "plt.xticks([])\n",
    "\n",
    "plt.subplot(233)\n",
    "sns.boxplot(x=train_df['Survived'], y=train_df['FamilySize'], showmeans = True, meanline = True, fliersize=4)\n",
    "plt.ylabel('Family Size')\n",
    "plt.ylim(-0.5,12.5)\n",
    "plt.xlabel('')\n",
    "plt.xticks([])\n",
    "\n",
    "plt.subplot(234)\n",
    "sns.histplot(data=train_df, x='Fare', hue='Survived', bins=30)\n",
    "plt.xlabel('Fare')\n",
    "plt.ylabel('# of Passengers')\n",
    "plt.legend(['Survived','Died'])\n",
    "\n",
    "plt.subplot(235)\n",
    "sns.histplot(data=train_df, x='Age', hue='Survived', bins=30)\n",
    "plt.ylabel('')\n",
    "plt.legend('')\n",
    "\n",
    "plt.subplot(236)\n",
    "sns.histplot(data=train_df, x='FamilySize', hue='Survived', bins=10)\n",
    "plt.xlabel('Family Size')\n",
    "plt.ylabel('')\n",
    "plt.legend('')\n",
    "\n",
    "# Title and subtitle\n",
    "plt.subplot(231)\n",
    "plt.text(s=\"Fare, Age, and Family Size distributions\", ha='left', x=-0.5, y=600, fontsize=18, fontweight='bold', color=darks[1])\n",
    "plt.title(\"by Survival Outcome\", loc='left', fontsize=12, y=1, color=darks[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-29T04:39:40.532484Z",
     "iopub.status.busy": "2022-05-29T04:39:40.532059Z",
     "iopub.status.idle": "2022-05-29T04:39:41.982316Z",
     "shell.execute_reply": "2022-05-29T04:39:41.981462Z",
     "shell.execute_reply.started": "2022-05-29T04:39:40.532451Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create Figure and subplots\n",
    "fig, axs = plt.subplots(2, 3, figsize=(12,10), sharey=True)\n",
    "\n",
    "# Plot\n",
    "sns.barplot(data=train_df, x='Embarked', y='Survived', ax=axs[0,0])\n",
    "sns.barplot(data=train_df, x='Pclass', y='Survived', ax=axs[0,1])\n",
    "sns.barplot(data=train_df, x='FamilyAboard', y='Survived', ax=axs[0,2])\n",
    "sns.pointplot(data=train_df, x='AgeBin', y='Survived', ax=axs[1,0])\n",
    "sns.pointplot(data=train_df, x='FareBin', y='Survived', ax=axs[1,1])\n",
    "sns.pointplot(data=train_df, x='FamilySize', y='Survived', ax=axs[1,2])\n",
    "\n",
    "# Format\n",
    "axs[0,1].set(xlabel='Passenger Class', ylabel='')\n",
    "axs[0,2].set(xlabel='Family Aboard', ylabel='',  xticklabels=['No','Yes'])\n",
    "axs[1,0].set(xlabel='Age', \n",
    "             xticklabels=['0-17','18-29','30-39','40-49','50-64', '65+'],\n",
    "             ylim=(-0.01,0.89))\n",
    "axs[1,0].tick_params(axis='x',rotation=70)\n",
    "axs[1,1].set(ylabel='', xlabel='Fare', xticklabels=['0-7.85','7.85-10.5','10.51-21.67','21.68-39.68','39.69-512.33'])\n",
    "axs[1,1].tick_params(axis='x', rotation=70)\n",
    "axs[1,2].set(xlabel='Family Size',ylabel='')\n",
    "\n",
    "fig.suptitle('Survival Rates by Feature', fontsize=18, fontweight=600, x=0.52, y=0.9, color=darks[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-29T04:39:41.983921Z",
     "iopub.status.busy": "2022-05-29T04:39:41.983683Z",
     "iopub.status.idle": "2022-05-29T04:39:42.950239Z",
     "shell.execute_reply": "2022-05-29T04:39:42.949636Z",
     "shell.execute_reply.started": "2022-05-29T04:39:41.983892Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,3,figsize=(12,6))\n",
    "\n",
    "sns.boxplot(data=train_df, x='Pclass', y='Fare', hue='Survived', ax=axs[0])\n",
    "axs[0].set(xlabel='Passanger Class', ylim=(-5,525))\n",
    "plt.legend(['Survived','Died'])\n",
    "\n",
    "sns.violinplot(data=train_df, x='Pclass', y='Age', hue='Survived', split=True, ax=axs[1])\n",
    "axs[1].set(xlabel='', ylim=(-1,100.5))\n",
    "axs[1].legend('')\n",
    "\n",
    "sns.boxplot(data=train_df, x='Pclass', y='FamilySize', hue='Survived', ax=axs[2])\n",
    "axs[2].set(xlabel='', ylim=(-0.1,12.1))\n",
    "axs[2].legend('')\n",
    "\n",
    "# Title and Subtitle\n",
    "xmin, xmax = axs[0].get_xlim()\n",
    "ymin, ymax = axs[0].get_ylim()\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.text(s='Fare, Age, and Family Size Distributions',\n",
    "         x=xmin, y=ymax*1.18, fontsize=18, fontweight='bold', color=darks[1])\n",
    "plt.title(\"by on Passenger Class and Survival Outcome\", loc='left', fontsize=12, y=1.1, color=darks[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-29T04:39:42.953331Z",
     "iopub.status.busy": "2022-05-29T04:39:42.952432Z",
     "iopub.status.idle": "2022-05-29T04:39:43.803466Z",
     "shell.execute_reply": "2022-05-29T04:39:43.802634Z",
     "shell.execute_reply.started": "2022-05-29T04:39:42.953284Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(12,6), sharey=True)\n",
    "\n",
    "sns.barplot(data=train_df, x='Pclass', y='Survived', hue='Sex', ax=axs[0])\n",
    "axs[0].set(xlabel='Passenger Class')\n",
    "\n",
    "sns.barplot(\n",
    "    data=train_df,\n",
    "    x='FamilyAboard',\n",
    "    y='Survived',\n",
    "    hue='Sex',\n",
    "    ax=axs[1])\n",
    "axs[1].set(xlabel='Family Aboard', ylabel='')\n",
    "axs[1].legend('')\n",
    "\n",
    "sns.barplot(\n",
    "    data=train_df,\n",
    "    x='Embarked',\n",
    "    y='Survived',\n",
    "    hue='Sex',\n",
    "    ax=axs[2])\n",
    "axs[2].set(ylabel='')\n",
    "axs[2].legend('')\n",
    "\n",
    "# Title and Subtitle\n",
    "xmin, xmax = axs[0].get_xlim()\n",
    "ymin, ymax = axs[0].get_ylim()\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.text(s='Survival Rates by Sex',\n",
    "         x=xmin,\n",
    "         y=ymax*1.1,\n",
    "         fontsize=18,\n",
    "         fontweight='bold',\n",
    "         color=darks[1])\n",
    "plt.title(\"by features Passenger Class, Family Aboard, and Embarked\",\n",
    "          loc='left',\n",
    "          fontsize=12,\n",
    "          y=1.04,\n",
    "          color=darks[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-29T04:39:43.805150Z",
     "iopub.status.busy": "2022-05-29T04:39:43.804921Z",
     "iopub.status.idle": "2022-05-29T04:39:44.983257Z",
     "shell.execute_reply": "2022-05-29T04:39:44.982627Z",
     "shell.execute_reply.started": "2022-05-29T04:39:43.805121Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2, figsize=(15,6), gridspec_kw={'width_ratios':[1,3]}, sharey=True)\n",
    "\n",
    "sns.pointplot(data=train_df, x='FamilySize', y='Survived', hue='Sex', ax=axs[0])\n",
    "axs[0].set(xlabel='Family Size')\n",
    "\n",
    "sns.barplot(data=train_df, x='AgeBin', y='Survived', hue='Sex', ax=axs[1])\n",
    "axs[1].set(\n",
    "    xlabel='Age', ylabel='', \n",
    "    xticklabels=['0-17','18-29','30-39','40-49','50-64', '65+'],\n",
    "    ylim=(-0.01,0.89))\n",
    "axs[1].tick_params(axis='x', rotation=30)\n",
    "axs[1].legend('')\n",
    "\n",
    "fig.suptitle('Survival Rates by Family Size and Age', fontsize=18, fontweight=600, x=0.48, y=1.02, color=darks[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-29T04:39:44.984390Z",
     "iopub.status.busy": "2022-05-29T04:39:44.984147Z",
     "iopub.status.idle": "2022-05-29T04:39:46.871378Z",
     "shell.execute_reply": "2022-05-29T04:39:46.870587Z",
     "shell.execute_reply.started": "2022-05-29T04:39:44.984361Z"
    }
   },
   "outputs": [],
   "source": [
    "grid = sns.FacetGrid(train_df, col='Pclass', row='Sex', hue='Survived')\n",
    "plt.figure(figsize=(14,8))\n",
    "grid.map(plt.hist, 'Age', bins=20, alpha=0.7)\n",
    "grid.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-29T04:39:46.873339Z",
     "iopub.status.busy": "2022-05-29T04:39:46.872876Z",
     "iopub.status.idle": "2022-05-29T04:39:47.168470Z",
     "shell.execute_reply": "2022-05-29T04:39:47.167618Z",
     "shell.execute_reply.started": "2022-05-29T04:39:46.873297Z"
    }
   },
   "outputs": [],
   "source": [
    "# Rainplot\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "ax=pt.half_violinplot(\n",
    "    data=train_df, x='Age', y='Pclass', palette=colors_default, orient='h', alpha=0.7,\n",
    "    bw=.2, linewidth=1,cut=0., scale=\"area\", width=.8, inner=None)\n",
    "ax=sns.stripplot(\n",
    "    data=train_df, x='Age', y='Pclass', palette=colors_default, orient='h',\n",
    "    edgecolor=\"white\",size=2,jitter=1,zorder=0)\n",
    "ax=sns.boxplot(\n",
    "    data=train_df, x='Age', y='Pclass', color=\"black\",orient='h',\n",
    "    width=.15,zorder=10,showcaps=True,boxprops={'facecolor':'none', \"zorder\":10},\n",
    "    showfliers=False,whiskerprops={'linewidth':2, \"zorder\":10},saturation=1)\n",
    "\n",
    "fig.suptitle(\n",
    "    'Age Distribution by Passenger Class',\n",
    "    fontsize=18,\n",
    "    fontweight=600,\n",
    "    x=0.51,\n",
    "    y=0.96,\n",
    "    color=darks[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-29T04:39:47.169806Z",
     "iopub.status.busy": "2022-05-29T04:39:47.169544Z",
     "iopub.status.idle": "2022-05-29T04:39:47.449596Z",
     "shell.execute_reply": "2022-05-29T04:39:47.448675Z",
     "shell.execute_reply.started": "2022-05-29T04:39:47.169770Z"
    }
   },
   "outputs": [],
   "source": [
    "# Below is a higher level method of creating a rainplot, requiring less code\n",
    "fig, axs = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "axs=pt.RainCloud(\n",
    "    data=train_df, x='Survived', y='Age', alpha=0.7,\n",
    "    bw=0.2, ax = axs, orient = 'h', palette=colors_default, dodge=True)\n",
    "\n",
    "fig.suptitle(\n",
    "    'Age Distribution by Survival',\n",
    "    fontsize=18,\n",
    "    fontweight=600,\n",
    "    x=0.51,\n",
    "    y=0.96,\n",
    "    color=darks[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-29T04:39:47.452123Z",
     "iopub.status.busy": "2022-05-29T04:39:47.451860Z",
     "iopub.status.idle": "2022-05-29T04:39:48.123474Z",
     "shell.execute_reply": "2022-05-29T04:39:48.122632Z",
     "shell.execute_reply.started": "2022-05-29T04:39:47.452092Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(\n",
    "    train_df.corr(), \n",
    "    cmap = sns.diverging_palette(10, 220, as_cmap=True, l=45, s=90), \n",
    "    annot=True, \n",
    "    square=True,\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    "    cbar_kws={'shrink':.75 },\n",
    "    linewidths=0.08, linecolor='white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>4. Model and Predict Data</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 4.0 Prepare Data</h3>\n",
    "\n",
    "<h4>Convert</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-29T04:39:48.125074Z",
     "iopub.status.busy": "2022-05-29T04:39:48.124756Z",
     "iopub.status.idle": "2022-05-29T04:39:48.151766Z",
     "shell.execute_reply": "2022-05-29T04:39:48.150883Z",
     "shell.execute_reply.started": "2022-05-29T04:39:48.125042Z"
    }
   },
   "outputs": [],
   "source": [
    "for df in [train_df, test_df]:\n",
    "    # Create dummy variables for categorical columns\n",
    "    df = pd.concat([df, pd.get_dummies(df[['Sex','Embarked','Title', 'Sex']], drop_first=True)], axis=1)\n",
    "    \n",
    "    # Remove original categorical columns\n",
    "    df.drop(['Sex', 'Embarked', 'Title'], axis=1, inplace=True)\n",
    "    \n",
    "    # Drop bin columns used for visualization. This data is already present in numerical columns\n",
    "    df.drop(['AgeBin', 'FareBin'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Create</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-29T04:39:48.153365Z",
     "iopub.status.busy": "2022-05-29T04:39:48.153138Z",
     "iopub.status.idle": "2022-05-29T04:39:48.176110Z",
     "shell.execute_reply": "2022-05-29T04:39:48.175015Z",
     "shell.execute_reply.started": "2022-05-29T04:39:48.153337Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create dummy variables for categorical columns\n",
    "train_df = pd.concat([train_df, pd.get_dummies(train_df[['Sex','Embarked','Title']], drop_first=True)], axis=1)\n",
    "test_df = pd.concat([test_df, pd.get_dummies(test_df[['Sex','Embarked','Title']], drop_first=True)], axis=1)\n",
    "\n",
    "# Drop original categorical columns. Drop bin columns used for visualization--data is already present in numerical columns\n",
    "train_df.drop(['Sex', 'Embarked', 'Title', 'AgeBin', 'FareBin'], axis=1, inplace=True)\n",
    "test_df.drop(['Sex', 'Embarked', 'Title', 'AgeBin', 'FareBin'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Split</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-29T04:39:48.178112Z",
     "iopub.status.busy": "2022-05-29T04:39:48.177750Z",
     "iopub.status.idle": "2022-05-29T04:39:48.188783Z",
     "shell.execute_reply": "2022-05-29T04:39:48.187607Z",
     "shell.execute_reply.started": "2022-05-29T04:39:48.178077Z"
    }
   },
   "outputs": [],
   "source": [
    "X=train_df.drop('Survived', axis=1)\n",
    "y=train_df['Survived']\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 4.1 Random Forest Classifier</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"width: 440px; height: 221px;\" src=\"https://www.explorium.ai/wp-content/uploads/2019/12/Decision-Trees-2.png\" align=\"Right\">\n",
    "\n",
    "Random Forest Classifiers are upgraded versions of Decision Trees by creating an ensemble of many models using the method of bagging (or \"Bootstrap Aggregation\").\n",
    "\n",
    "The foundational Decision Trees Classifier is an intuitive model. The model will first split the data/rows/samples by each its value for one particular column/field/variable/feature/variable (let's stick \"sample\" with \"variable\"). The goal of this split is to make each sample subsethave more homogenous values for the target variable. After splitting the samples at this <i>root node</i>, the model repeats this process for each <i>branch</i> by utilizing additional variables as <i>decision nodes</i>. The goal is to end with <i>leaf nodes</i> that accurately classify the samples in that node as one particular class.\n",
    "\n",
    "A key concern here is <i>which feature should be used for each decision?</i> This can be determined by a variety of metrics, but the most common are Entropy and Gini Index. In brief...\n",
    " \n",
    "<p style=\"padding-left: 10px;\">\n",
    "$Gini = 1-\\sum_{i=1}^{n}{(p_{i})^{2}}$<br>\n",
    "$Entropy = -\\sum_{i=1}^{n}{p_{i} log_{2}(p_i)}$</p>\n",
    "\n",
    "where $P_{i}$ denotes the porportion of samples belonging to class $i$ for that particular node. In other words, the probability of data being classified for a distinct class. These metrics range from 0 to 1, where 0 is best (most \"pure\" for gini and most \"ordered\" for Entropy). The Gini Index tends to favor nodes with larger partitions and Entropy tends to favor nodes with smaller partitions and more distinct classes.\n",
    "\n",
    "The greatest downfall to a Decision Tree model is it tends to overfit because it is highly dependent on the data the model is trained with. There are several parameters to help counteract this, such as min and max number of samples before splitting, min and max tree depth, etc. One of the greatest methods to prevent overfitting is <i>bagging</i> which creates the <b><i>Random Forest Classifier</i></b>.\n",
    "\n",
    "RFC creates an ensemble of Decision Trees such that each one is trained using only a subset (wih replacement) of the entire dataset--this is known as bagging. Additionally, each Decision Tree can be limited to making decisions based on only a subset of all variables. These two restrictions help establish \"a large number of relatively uncorrelated models (trees) operating as a committee [that outperforms] any of the individual constituent models.\" These Decision Trees are created in parallel so the Random Forest Classifier can make predicitions based on the average of the trees. Note that the SciKit-Learn RFC uses subsets are selected with replacement, but are always the same size as the original sample set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-29T04:39:48.813143Z",
     "iopub.status.busy": "2022-05-29T04:39:48.812469Z",
     "iopub.status.idle": "2022-05-29T04:42:04.105007Z",
     "shell.execute_reply": "2022-05-29T04:42:04.104230Z",
     "shell.execute_reply.started": "2022-05-29T04:39:48.813103Z"
    }
   },
   "outputs": [],
   "source": [
    "# RFC Benchmark Model\n",
    "rfc = RandomForestClassifier(random_state=101)\n",
    "rfc.fit(X_train,y_train)\n",
    "\n",
    "# Random Forest Classifier with Random Feature Elimiation\n",
    "rfc_rfe = RFECV(\n",
    "    RandomForestClassifier(random_state=101),\n",
    "    step = 1,\n",
    "    scoring = 'roc_auc',\n",
    "    cv=4)\n",
    "\n",
    "rfc_rfe.fit(X_train, y_train)\n",
    "selected_features = X.columns.values[rfc_rfe.get_support()]\n",
    "\n",
    "rfc_rfe = RandomForestClassifier(random_state=101)\n",
    "rfc_rfe = rfc_rfe.fit(X_train[selected_features], y_train)\n",
    "\n",
    "# Use GridSearchCV to improve Random Forest Classifier parameters\n",
    "rfc_params = {\n",
    "    'n_estimators':[100, 150, 200],\n",
    "    'criterion': ['gini', 'entropy'],  \n",
    "    'max_depth': [4,6,8,10,None],\n",
    "    'min_samples_split': [2,5,10,20],\n",
    "    'random_state': [101]}\n",
    "\n",
    "rfc_tuned = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=101),\n",
    "    rfc_params,verbose=1,\n",
    "    scoring = 'roc_auc',\n",
    "    cv=4)\n",
    "\n",
    "rfc_tuned.fit(X_train[selected_features], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print best parameters and selected features\n",
    "print('Selected Features:', selected_features,'\\n')\n",
    "print('Best Parameters:', rfc_tuned.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>4.2 Support Vector Machine Classifier</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"width: 414px; height: 180px;\" src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSDlYsiPUl636bhR-MBaBDkSgOQdwEJAmXdKg&usqp=CAU\" align=\"Right\">\n",
    "In brief, Support Vector Machines represent samples as points in space. The goal is to identify a hyperplane--known as the <i>Decision Boundary</i>--that separates the distinct classes as clearly as possible (as large of a separation as possible). If these classes are not linearly separable, then the <i>Kernel Trick</i> will map the data to another dimension to improve separability. SVMs tend to perform at a higher speed and performance when samples are limited than other modern models, like Neural Networks. For large amounts of data, however, these models can become very computationally expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-29T04:42:04.106857Z",
     "iopub.status.busy": "2022-05-29T04:42:04.106634Z",
     "iopub.status.idle": "2022-05-29T04:42:23.000990Z",
     "shell.execute_reply": "2022-05-29T04:42:22.999930Z",
     "shell.execute_reply.started": "2022-05-29T04:42:04.106831Z"
    }
   },
   "outputs": [],
   "source": [
    "# SVC Benchmark Model\n",
    "svc = SVC(probability=True)\n",
    "svc.fit(X_train,y_train)\n",
    "\n",
    "# SVC with tuned parameters using GridSearchCV\n",
    "svc_params = {'C':[0.1,1,10,100,1000, 10000], 'gamma':[1,0.1,0.01,0.001,0.0001]}\n",
    "svc_tuned = GridSearchCV(SVC(),svc_params,verbose=1, scoring='roc_auc', cv=4)\n",
    "svc_tuned.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>4.3 XGBoost</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bagging method was used in the Random Forest Classifier to combat overfitting. Another method to control bias-variance tradeoff that utilizes models built in <u>series</u> instead of parallel is <i>boosting</i>. For Gradient Boosting, a loss function is used to detect the residuals of the individual <i>weak learners</i>, such as Decision Trees. Gradient Descent is utilized to minimize the loss function when creating the next model. The final model aggregates the result of each step (creates ensemble) and thus a strong learner is obtained.\n",
    "\n",
    "\n",
    "Gradient Boosted Decision Trees (GBDTs) iteratively train an ensemble of shallow decision trees, with each iteration using the error residuals of the previous model to fit the next model. The final prediction is a weighted sum of all of the tree predictions. XGBoost is short for eXtreme Gradient Boosting and predominantly aims to improve speed and performance of the Gradient Boost with all the same foundational theory.\n",
    "\n",
    "Some quick tips for the XGBoost library:\n",
    "* can handle missing values if they are not dealt with in preprocessing\n",
    "* cannot handle categorical features\n",
    "* Transform data to DMatrix structure, which is an optimized data structure that provides better memory efficiency and training speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-29T04:42:23.002606Z",
     "iopub.status.busy": "2022-05-29T04:42:23.002361Z",
     "iopub.status.idle": "2022-05-29T04:42:23.463268Z",
     "shell.execute_reply": "2022-05-29T04:42:23.462660Z",
     "shell.execute_reply.started": "2022-05-29T04:42:23.002563Z"
    }
   },
   "outputs": [],
   "source": [
    "# Store data in DMatrix object\n",
    "ddata = xgb.DMatrix(data=X, label=y)\n",
    "dtrain = xgb.DMatrix(data=X_train, label=y_train)\n",
    "dval = xgb.DMatrix(data=X_val, label=y_val)\n",
    "\n",
    "# XGB Benchmark Model\n",
    "# Specify parameters\n",
    "num_round = 500 # Early stopping will be applied\n",
    "xgb_params = {\n",
    "    'max_depth':2,\n",
    "    'learning_rate':1,\n",
    "    'objective':'binary:logistic',\n",
    "    'eval_metric':['logloss','auc']} # last metric listed used for early_stopping\n",
    "\n",
    "# Cross Validation\n",
    "xgb_cv = xgb.cv(\n",
    "    xgb_params,\n",
    "    ddata,\n",
    "    num_round,\n",
    "    nfold=4,\n",
    "    early_stopping_rounds=10,\n",
    "    seed=101,\n",
    "    callbacks=[xgb.callback.EvaluationMonitor(show_stdv=True)]) # Will return mean+std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two main methods to combat overfitting a model:\n",
    "\n",
    "1. Directly control model complexity.\n",
    "    * For a tree model, this includes max_depth, min_child_weight and gamma (min split loss).\n",
    "2. Add randomness to make training robust to noise.\n",
    "    * For a tree model, this includes subsample and colsample_bytree.\n",
    "    * Reduce stepsize eta (\"learning rate\"). If this is done, num_round should be increased.\n",
    "\n",
    "The training data (including validation, just not the final test data) is moderately imbalanced (38%). Since the model is concerned with predicting the correct outcome and not necessarily the predicting the correct probabilities, data will be balanced (see <a href=\"https://xgboost.readthedocs.io/en/stable/tutorials/param_tuning.html?highlight=balance\">XGB documentation</a>).\n",
    "\n",
    "XGBoost library does not have any GridSearchCV sort of function. It is absolutely possuble to use GridSearchCV, but it will not accept a DMatrix object for the data, greatly helps the speed of XGB. For this reason, xgb.cv() will be looped to achieve the same goal as GridSearchCV, but faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-29T04:42:29.257983Z",
     "iopub.status.busy": "2022-05-29T04:42:29.257690Z"
    }
   },
   "outputs": [],
   "source": [
    "ddata = xgb.DMatrix(data=X, label=y)\n",
    "dtrain = xgb.DMatrix(data=X_train, label=y_train)\n",
    "dval = xgb.DMatrix(data=X_val, label=y_val)\n",
    "\n",
    "params_current = {\n",
    "    'objective':'binary:logistic',\n",
    "    'eval_metric':['logloss','auc']}\n",
    "\n",
    "# Define preprocessing function to balance data with 'scale_pos_weight'\n",
    "def fpreproc(dtrain, dtest, param):\n",
    "    label = dtrain.get_label()\n",
    "    ratio = float(np.sum(label == 0)) / np.sum(label == 1)\n",
    "    param['scale_pos_weight'] = ratio\n",
    "    return (dtrain, dtest, param)\n",
    "\n",
    "auc_xgb_best = 0\n",
    "params_xgb_best = None\n",
    "\n",
    "for gam in [0,0.5,1]:\n",
    "    params_current['gamma'] = gam\n",
    "    \n",
    "    for min_child in [0,15,35]:\n",
    "        params_current['min_child_weight'] = min_child\n",
    "        \n",
    "        for max_dep in [2,4,6,None]:\n",
    "            params_current['max_depth'] = max_dep\n",
    "            \n",
    "            for learn in [0.025,0.05,0.1,0.5,1]:\n",
    "                params_current['learning_rate'] = learn\n",
    "                \n",
    "                for alpha in [0,0.5,1]:\n",
    "                    params_current['reg_alpha'] = alpha\n",
    "                    \n",
    "                    for lamb in [0,0.5,1]:\n",
    "                        params_current['reg_lambda'] = lamb\n",
    "                \n",
    "                        xgb_tuning = xgb.cv(\n",
    "                            params_current,\n",
    "                            ddata,\n",
    "                            num_round,\n",
    "                            nfold=4,\n",
    "                            early_stopping_rounds=10,\n",
    "                            fpreproc=fpreproc,\n",
    "                            seed=101,\n",
    "                            # return mean+std\n",
    "                            callbacks=[xgb.callback.EvaluationMonitor(show_stdv=True)]) \n",
    "\n",
    "                        auc_result = xgb_tuning.iloc[-1]['test-auc-mean']\n",
    "\n",
    "                        if  auc_result > auc_xgb_best:\n",
    "                            auc_xgb_best = auc_result\n",
    "                            params_xgb_best = params_current.copy()\n",
    "                            xgb_best = xgb_tuning\n",
    "                            \n",
    "# Fit model to best parameters for final model comparison\n",
    "xgb_tuned = xgb.train(\n",
    "    params_xgb_best,\n",
    "    dtrain,\n",
    "    num_round,\n",
    "    early_stopping_rounds=10,\n",
    "    evals=[(dval,'Validation')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For one last model, let's tune a boosted version of the top performing RFC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data with feature selection to DMatrix\n",
    "ddata_feat = xgb.DMatrix(data=X[selected_features], label=y)\n",
    "dtrain_feat = xgb.DMatrix(data=X_train[selected_features], label=y_train)\n",
    "dval_feat = xgb.DMatrix(data=X_val[selected_features], label=y_val)\n",
    "\n",
    "auc_xgbrf_best = 0\n",
    "params_xgbrf_best = None\n",
    "\n",
    "# Keep params of top RFC. Only boosting parameters will be tuned\n",
    "for gamma in [0,0.5,0.75,1]:    \n",
    "    for min_child in [0,1,2,3]:            \n",
    "        for learn in [0.01, 0.05, 0.1]:\n",
    "            for alpha in [0,0.5,0.75,1]:          \n",
    "                for lamb in [0,0.5,1]:\n",
    "                    \n",
    "                    xgbrf_tuning = xgb.XGBRFClassifier(\n",
    "                        n_estimators=100,\n",
    "                        max_depth=8,\n",
    "                        min_samples_split=5,\n",
    "                        criterion ='gini',\n",
    "                        learning_rate=learn,\n",
    "                        gamma=gamma,\n",
    "                        min_child_weight=min_child,\n",
    "                        reg_alpha=alpha,\n",
    "                        reg_lambda=lamb,\n",
    "                        use_label_encoder=False)\n",
    "\n",
    "                    xgbrf_tuning.fit(\n",
    "                        X_train[selected_features],\n",
    "                        y_train,\n",
    "                        eval_set=[(X_val[selected_features], y_val)],\n",
    "                        eval_metric=['logloss','auc'])\n",
    "\n",
    "                    auc_result = xgbrf_tuning.evals_result_['validation_0']['auc'][0]\n",
    "\n",
    "                    if  auc_result > auc_xgbrf_best:\n",
    "                        auc_xgbrf_best = auc_result\n",
    "                        params_xgbrf_best = xgbrf_tuning.get_params\n",
    "                        xgbrf_tuned = xgbrf_tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ['RFC', 'RFC Feature Selection','RFC Tuned','RFC Boosted',\n",
    "    'SVC','SVC Tuned','XGB', 'XGB Tuned']\n",
    "\n",
    "metric_result = [\n",
    "    metrics.roc_auc_score(y_val, rfc.predict_proba(X_val)[:,1]),\n",
    "    metrics.roc_auc_score(y_val, rfc_rfe.predict_proba(X_val[selected_features])[:,1]),\n",
    "    metrics.roc_auc_score(y_val, rfc_tuned.predict_proba(X_val[selected_features])[:,1]),   \n",
    "    auc_xgbrf_best,\n",
    "    metrics.roc_auc_score(y_val, svc.predict_proba(X_val)[:,1]),\n",
    "    svc_tuned.best_score_,\n",
    "    xgb_cv.iloc[-1]['test-auc-mean'],\n",
    "    auc_xgb_best]\n",
    "\n",
    "pd.DataFrame(metric_result, model, columns =['AUC_ROC'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Resources, References, Documentation:</h3><hr>\n",
    "\n",
    "XGBoost resources: \n",
    "* <a href=\"https://xgboost.readthedocs.io/en/stable/python/python_intro.html#training\">XGB Documentation: Python Package Introduction</a>\n",
    "* <a href=\"https://xgboost.readthedocs.io/en/stable/tutorials/param_tuning.html\">XGB Documentation: Notes on Parameter Tuning</a>\n",
    "* <a href=\"https://towardsdatascience.com/binary-classification-xgboost-hyperparameter-tuning-scenarios-by-non-exhaustive-grid-search-and-c261f4ce098d\">TowardsDataScience: XGBoost Hyperparameter Tuning</a>\n",
    "* <a href=\"https://www.kaggle.com/prashant111/xgboost-k-fold-cv-feature-importance\">Prashant Banerjee's Kaggle Notebook</a>\n",
    "* <a href=\"https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\">Aarshay Jain: Parameter Tuning in XGboost</a>\n",
    "* <a href=\"https://debuggercafe.com/a-guide-to-xgboost-in-python/\">DeBuggerCafe: A Guide to XGBoost in Python</a>\n",
    "* <a href=\"https://github.com/fmfn/BayesianOptimization/blob/master/README.md\">Bayesian Optimizer for XGB</a>\n",
    "\n",
    "\n",
    "Some visualization inspiration from fivethirtyeight:\n",
    "* <a href=\"https://fivethirtyeight.com/features/the-52-best-and-weirdest-charts-we-made-in-2016/\">2016 Best Charts</a>\n",
    "* <a href=\"https://fivethirtyeight.com/features/the-56-best-and-weirdest-charts-we-made-in-2019/\">2019 Best Charts</a>\n",
    "* <a href=\"https://fivethirtyeight.com/features/the-40-weirdest-and-best-charts-we-made-in-2020/\">2020 Best Charts</a>\n",
    "* <a href=\"https://github.com/matplotlib/matplotlib/blob/38be7aeaaac3691560aeadafe46722dda427ef47/lib/matplotlib/mpl-data/stylelib/fivethirtyeight.mplstyle\">fivethirtyeight default styling sheet</a>\n",
    "* <a href=\"https://www.dataquest.io/blog/making-538-plots/\">Example of making a simple fivethirtyeight style</a>\n",
    "\n",
    "Miscellaneous:\n",
    "* <a href=\"https://www.kaggle.com/ldfreeman3/a-data-science-framework-to-achieve-99-accuracy#Step-3:-Prepare-Data-for-Consumption\">LD Freeman's Kaggle Notebook</a>\n",
    "* <a href=\"https://towardsdatascience.com/understanding-random-forest-58381e0602d2\">TowardsDataScience: Random Forest Classifier</a>\n",
    "* <a href=\"https://matplotlib.org/stable/tutorials/introductory/customizing.html\">Documentation for customizing with stylesheets and rcParams</a>\n",
    "* <a href=\"https://wellcomeopenresearch.org/articles/4-63/v2\">Why Use Rainplots</a>\n",
    "* <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html\">To Visualize Decision Tress, use GraphViz</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python-myenv",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
